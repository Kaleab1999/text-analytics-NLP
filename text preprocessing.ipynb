{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf35a0e0",
   "metadata": {},
   "source": [
    "perfoming some text preprocessing techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d59bfb",
   "metadata": {},
   "source": [
    "installing nltk library to apply NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6115fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\kiit\\anaconda3\\lib\\site-packages (3.6.5)\n",
      "Requirement already satisfied: click in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171f2ff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed6523ce",
   "metadata": {},
   "source": [
    "importting necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7b7a60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e20531c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\KIIT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ba5bd8",
   "metadata": {},
   "source": [
    "to use stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64fd7cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\KIIT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136e3814",
   "metadata": {},
   "source": [
    "loading text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd996308",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Here’s to the crazy ones, the misfits, the rebels, \n",
    "the troublemakers, the round pegs in the square holes. The ones who see things differently — they’re not fond of rules. You can quote them, disagree with them, glorify or vilify them, but the only thing you can’t do is ignore them because they change things. They push the human race forward, and while some may see them as the crazy ones, we see genius, because the ones who are crazy enough to think\n",
    "that they can change the world, are the ones who do.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6750bfde",
   "metadata": {},
   "source": [
    "### Tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3240ac9e",
   "metadata": {},
   "source": [
    "Tokenization is the process of splitting the text into smaller unit called tokens(words, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82f22a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here’s',\n",
       " 'to',\n",
       " 'the',\n",
       " 'crazy',\n",
       " 'ones,',\n",
       " 'the',\n",
       " 'misfits,',\n",
       " 'the',\n",
       " 'rebels,',\n",
       " 'the',\n",
       " 'troublemakers,',\n",
       " 'the',\n",
       " 'round',\n",
       " 'pegs',\n",
       " 'in',\n",
       " 'the',\n",
       " 'square',\n",
       " 'holes.',\n",
       " 'The',\n",
       " 'ones',\n",
       " 'who',\n",
       " 'see',\n",
       " 'things',\n",
       " 'differently',\n",
       " '—',\n",
       " 'they’re',\n",
       " 'not',\n",
       " 'fond',\n",
       " 'of',\n",
       " 'rules.',\n",
       " 'You',\n",
       " 'can',\n",
       " 'quote',\n",
       " 'them,',\n",
       " 'disagree',\n",
       " 'with',\n",
       " 'them,',\n",
       " 'glorify',\n",
       " 'or',\n",
       " 'vilify',\n",
       " 'them,',\n",
       " 'but',\n",
       " 'the',\n",
       " 'only',\n",
       " 'thing',\n",
       " 'you',\n",
       " 'can’t',\n",
       " 'do',\n",
       " 'is',\n",
       " 'ignore',\n",
       " 'them',\n",
       " 'because',\n",
       " 'they',\n",
       " 'change',\n",
       " 'things.',\n",
       " 'They',\n",
       " 'push',\n",
       " 'the',\n",
       " 'human',\n",
       " 'race',\n",
       " 'forward,',\n",
       " 'and',\n",
       " 'while',\n",
       " 'some',\n",
       " 'may',\n",
       " 'see',\n",
       " 'them',\n",
       " 'as',\n",
       " 'the',\n",
       " 'crazy',\n",
       " 'ones,',\n",
       " 'we',\n",
       " 'see',\n",
       " 'genius,',\n",
       " 'because',\n",
       " 'the',\n",
       " 'ones',\n",
       " 'who',\n",
       " 'are',\n",
       " 'crazy',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'think',\n",
       " 'that',\n",
       " 'they',\n",
       " 'can',\n",
       " 'change',\n",
       " 'the',\n",
       " 'world,',\n",
       " 'are',\n",
       " 'the',\n",
       " 'ones',\n",
       " 'who',\n",
       " 'do.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#without using nltk library\n",
    "words=text.split()\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd4b114f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here',\n",
       " '’',\n",
       " 's',\n",
       " 'to',\n",
       " 'the',\n",
       " 'crazy',\n",
       " 'ones',\n",
       " ',',\n",
       " 'the',\n",
       " 'misfits',\n",
       " ',',\n",
       " 'the',\n",
       " 'rebels',\n",
       " ',',\n",
       " 'the',\n",
       " 'troublemakers',\n",
       " ',',\n",
       " 'the',\n",
       " 'round',\n",
       " 'pegs',\n",
       " 'in',\n",
       " 'the',\n",
       " 'square',\n",
       " 'holes',\n",
       " '.',\n",
       " 'The',\n",
       " 'ones',\n",
       " 'who',\n",
       " 'see',\n",
       " 'things',\n",
       " 'differently',\n",
       " '—',\n",
       " 'they',\n",
       " '’',\n",
       " 're',\n",
       " 'not',\n",
       " 'fond',\n",
       " 'of',\n",
       " 'rules',\n",
       " '.',\n",
       " 'You',\n",
       " 'can',\n",
       " 'quote',\n",
       " 'them',\n",
       " ',',\n",
       " 'disagree',\n",
       " 'with',\n",
       " 'them',\n",
       " ',',\n",
       " 'glorify',\n",
       " 'or',\n",
       " 'vilify',\n",
       " 'them',\n",
       " ',',\n",
       " 'but',\n",
       " 'the',\n",
       " 'only',\n",
       " 'thing',\n",
       " 'you',\n",
       " 'can',\n",
       " '’',\n",
       " 't',\n",
       " 'do',\n",
       " 'is',\n",
       " 'ignore',\n",
       " 'them',\n",
       " 'because',\n",
       " 'they',\n",
       " 'change',\n",
       " 'things',\n",
       " '.',\n",
       " 'They',\n",
       " 'push',\n",
       " 'the',\n",
       " 'human',\n",
       " 'race',\n",
       " 'forward',\n",
       " ',',\n",
       " 'and',\n",
       " 'while',\n",
       " 'some',\n",
       " 'may',\n",
       " 'see',\n",
       " 'them',\n",
       " 'as',\n",
       " 'the',\n",
       " 'crazy',\n",
       " 'ones',\n",
       " ',',\n",
       " 'we',\n",
       " 'see',\n",
       " 'genius',\n",
       " ',',\n",
       " 'because',\n",
       " 'the',\n",
       " 'ones',\n",
       " 'who',\n",
       " 'are',\n",
       " 'crazy',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'think',\n",
       " 'that',\n",
       " 'they',\n",
       " 'can',\n",
       " 'change',\n",
       " 'the',\n",
       " 'world',\n",
       " ',',\n",
       " 'are',\n",
       " 'the',\n",
       " 'ones',\n",
       " 'who',\n",
       " 'do',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#performing tokenization using nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "words_=word_tokenize(text)\n",
    "words_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8d8cf0",
   "metadata": {},
   "source": [
    "tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4971661a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here’s to the crazy ones, the misfits, the rebels, \\nthe troublemakers, the round pegs in the square holes.',\n",
       " 'The ones who see things differently — they’re not fond of rules.',\n",
       " 'You can quote them, disagree with them, glorify or vilify them, but the only thing you can’t do is ignore them because they change things.',\n",
       " 'They push the human race forward, and while some may see them as the crazy ones, we see genius, because the ones who are crazy enough to think\\nthat they can change the world, are the ones who do.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#performing tokenization(sentences) using nltk library\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sentences=sent_tokenize(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0259a4",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2310eb2",
   "metadata": {},
   "source": [
    "appling stemming on the sentences"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8001c7b0",
   "metadata": {},
   "source": [
    "stemming is the process of changing the worm to its root, there is 2 tecniques\n",
    "1. stemming: it might changes the word to meaning less word. we use this for sentilment analysis\n",
    "2. lemmitization: it changes the word to meaning ful word. we use this for chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94542c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here  :  here\n",
      "’  :  ’\n",
      "s  :  s\n",
      "to  :  to\n",
      "the  :  the\n",
      "crazy  :  crazi\n",
      "ones  :  one\n",
      ",  :  ,\n",
      "the  :  the\n",
      "misfits  :  misfit\n",
      ",  :  ,\n",
      "the  :  the\n",
      "rebels  :  rebel\n",
      ",  :  ,\n",
      "the  :  the\n",
      "troublemakers  :  troublemak\n",
      ",  :  ,\n",
      "the  :  the\n",
      "round  :  round\n",
      "pegs  :  peg\n",
      "in  :  in\n",
      "the  :  the\n",
      "square  :  squar\n",
      "holes  :  hole\n",
      ".  :  .\n",
      "The  :  the\n",
      "ones  :  one\n",
      "who  :  who\n",
      "see  :  see\n",
      "things  :  thing\n",
      "differently  :  differ\n",
      "—  :  —\n",
      "they  :  they\n",
      "’  :  ’\n",
      "re  :  re\n",
      "not  :  not\n",
      "fond  :  fond\n",
      "of  :  of\n",
      "rules  :  rule\n",
      ".  :  .\n",
      "You  :  you\n",
      "can  :  can\n",
      "quote  :  quot\n",
      "them  :  them\n",
      ",  :  ,\n",
      "disagree  :  disagre\n",
      "with  :  with\n",
      "them  :  them\n",
      ",  :  ,\n",
      "glorify  :  glorifi\n",
      "or  :  or\n",
      "vilify  :  vilifi\n",
      "them  :  them\n",
      ",  :  ,\n",
      "but  :  but\n",
      "the  :  the\n",
      "only  :  onli\n",
      "thing  :  thing\n",
      "you  :  you\n",
      "can  :  can\n",
      "’  :  ’\n",
      "t  :  t\n",
      "do  :  do\n",
      "is  :  is\n",
      "ignore  :  ignor\n",
      "them  :  them\n",
      "because  :  becaus\n",
      "they  :  they\n",
      "change  :  chang\n",
      "things  :  thing\n",
      ".  :  .\n",
      "They  :  they\n",
      "push  :  push\n",
      "the  :  the\n",
      "human  :  human\n",
      "race  :  race\n",
      "forward  :  forward\n",
      ",  :  ,\n",
      "and  :  and\n",
      "while  :  while\n",
      "some  :  some\n",
      "may  :  may\n",
      "see  :  see\n",
      "them  :  them\n",
      "as  :  as\n",
      "the  :  the\n",
      "crazy  :  crazi\n",
      "ones  :  one\n",
      ",  :  ,\n",
      "we  :  we\n",
      "see  :  see\n",
      "genius  :  geniu\n",
      ",  :  ,\n",
      "because  :  becaus\n",
      "the  :  the\n",
      "ones  :  one\n",
      "who  :  who\n",
      "are  :  are\n",
      "crazy  :  crazi\n",
      "enough  :  enough\n",
      "to  :  to\n",
      "think  :  think\n",
      "that  :  that\n",
      "they  :  they\n",
      "can  :  can\n",
      "change  :  chang\n",
      "the  :  the\n",
      "world  :  world\n",
      ",  :  ,\n",
      "are  :  are\n",
      "the  :  the\n",
      "ones  :  one\n",
      "who  :  who\n",
      "do  :  do\n",
      ".  :  .\n"
     ]
    }
   ],
   "source": [
    "# import these modules\n",
    "from nltk.stem import PorterStemmer\n",
    "  \n",
    "ps = PorterStemmer()\n",
    "for w in words_:\n",
    "    print(w, \" : \", ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807e5274",
   "metadata": {},
   "source": [
    "applying lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8386f18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here   Here\n",
      "’   ’\n",
      "s   s\n",
      "to   to\n",
      "the   the\n",
      "crazy   crazy\n",
      "ones   one\n",
      ",   ,\n",
      "the   the\n",
      "misfits   misfit\n",
      ",   ,\n",
      "the   the\n",
      "rebels   rebel\n",
      ",   ,\n",
      "the   the\n",
      "troublemakers   troublemaker\n",
      ",   ,\n",
      "the   the\n",
      "round   round\n",
      "pegs   peg\n",
      "in   in\n",
      "the   the\n",
      "square   square\n",
      "holes   hole\n",
      ".   .\n",
      "The   The\n",
      "ones   one\n",
      "who   who\n",
      "see   see\n",
      "things   thing\n",
      "differently   differently\n",
      "—   —\n",
      "they   they\n",
      "’   ’\n",
      "re   re\n",
      "not   not\n",
      "fond   fond\n",
      "of   of\n",
      "rules   rule\n",
      ".   .\n",
      "You   You\n",
      "can   can\n",
      "quote   quote\n",
      "them   them\n",
      ",   ,\n",
      "disagree   disagree\n",
      "with   with\n",
      "them   them\n",
      ",   ,\n",
      "glorify   glorify\n",
      "or   or\n",
      "vilify   vilify\n",
      "them   them\n",
      ",   ,\n",
      "but   but\n",
      "the   the\n",
      "only   only\n",
      "thing   thing\n",
      "you   you\n",
      "can   can\n",
      "’   ’\n",
      "t   t\n",
      "do   do\n",
      "is   is\n",
      "ignore   ignore\n",
      "them   them\n",
      "because   because\n",
      "they   they\n",
      "change   change\n",
      "things   thing\n",
      ".   .\n",
      "They   They\n",
      "push   push\n",
      "the   the\n",
      "human   human\n",
      "race   race\n",
      "forward   forward\n",
      ",   ,\n",
      "and   and\n",
      "while   while\n",
      "some   some\n",
      "may   may\n",
      "see   see\n",
      "them   them\n",
      "as   a\n",
      "the   the\n",
      "crazy   crazy\n",
      "ones   one\n",
      ",   ,\n",
      "we   we\n",
      "see   see\n",
      "genius   genius\n",
      ",   ,\n",
      "because   because\n",
      "the   the\n",
      "ones   one\n",
      "who   who\n",
      "are   are\n",
      "crazy   crazy\n",
      "enough   enough\n",
      "to   to\n",
      "think   think\n",
      "that   that\n",
      "they   they\n",
      "can   can\n",
      "change   change\n",
      "the   the\n",
      "world   world\n",
      ",   ,\n",
      "are   are\n",
      "the   the\n",
      "ones   one\n",
      "who   who\n",
      "do   do\n",
      ".   .\n"
     ]
    }
   ],
   "source": [
    "# import these modules\n",
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for wo in words_:\n",
    "    print(wo,' ', lemmatizer.lemmatize(wo))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e9f748",
   "metadata": {},
   "source": [
    "### removing Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4751d183",
   "metadata": {},
   "source": [
    "to remove stop words from this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae6a88d",
   "metadata": {},
   "source": [
    "stop words are words that are not significant in the sentences, like\n",
    "1. preposition: to, from, ... \n",
    "2. pronoun: he, she, it, ...\n",
    "3. articles: a, the, an\n",
    "4. conjunction: and, or, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7ecda2",
   "metadata": {},
   "source": [
    "listing stop words in English language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d6a8534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386068fa",
   "metadata": {},
   "source": [
    "apply stop words on stemmed words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbab03d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['here ’ crazi one , misfit , rebel , troublemak , round peg squar hole .',\n",
       " 'the one see thing differ — ’ fond rule .',\n",
       " 'you quot , disagre , glorifi vilifi , thing ’ ignor chang thing .',\n",
       " 'they push human race forward , may see crazi one , see geniu , one crazi enough think chang world , one .']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences=nltk.sent_tokenize(text)\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[ps.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)   \n",
    "sentences       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364b8c75",
   "metadata": {},
   "source": [
    "apply stop words on lemmitized word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec1044c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here ’ crazy one , misfit , rebel , troublemaker , round peg square hole .',\n",
       " 'The one see thing differently — ’ fond rule .',\n",
       " 'You quote , disagree , glorify vilify , thing ’ ignore change thing .',\n",
       " 'They push human race forward , may see crazy one , see genius , one crazy enough think change world , one .']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences=nltk.sent_tokenize(text)\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)   \n",
    "sentences        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2009ffb2",
   "metadata": {},
   "source": [
    "change the text to vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1863fdf9",
   "metadata": {},
   "source": [
    "1. bag of words:\n",
    "change the words to number as the existance number of the word in a sentence. eg. if 'Here' exist 2 times then it will be 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e35ecbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 1],\n",
       "       [1, 2, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 3, 0, 1, 0, 1, 0, 0,\n",
       "        0, 2, 0, 0, 1, 0, 1, 0, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer()\n",
    "x=cv.fit_transform(sentences).toarray()\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0496f4f1",
   "metadata": {},
   "source": [
    "2. TFIDF:\n",
    "change the words to number using formula of TF(term frequency) and IDF(inverce document frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93b2696d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.2623814 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.33279753,\n",
       "        0.33279753, 0.        , 0.        , 0.        , 0.33279753,\n",
       "        0.21242036, 0.33279753, 0.        , 0.        , 0.        ,\n",
       "        0.33279753, 0.33279753, 0.        , 0.        , 0.33279753,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.33279753,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.42068099, 0.        , 0.        ,\n",
       "        0.42068099, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.26851522, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.42068099, 0.33166972, 0.        ,\n",
       "        0.42068099, 0.        , 0.33166972, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.26124158, 0.        , 0.        , 0.33135182, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.33135182, 0.        ,\n",
       "        0.        , 0.        , 0.33135182, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.33135182, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.52248316, 0.        , 0.        ,\n",
       "        0.33135182, 0.        , 0.33135182],\n",
       "       [0.17964421, 0.35928842, 0.        , 0.        , 0.2278559 ,\n",
       "        0.        , 0.2278559 , 0.2278559 , 0.        , 0.        ,\n",
       "        0.        , 0.2278559 , 0.        , 0.2278559 , 0.        ,\n",
       "        0.43631241, 0.        , 0.2278559 , 0.        , 0.2278559 ,\n",
       "        0.        , 0.        , 0.        , 0.35928842, 0.        ,\n",
       "        0.        , 0.2278559 , 0.        , 0.2278559 , 0.        ,\n",
       "        0.        , 0.2278559 , 0.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf=TfidfVectorizer()\n",
    "y=tf.fit_transform(sentences).toarray()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe27564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd73cfed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
